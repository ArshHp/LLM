{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeqAXdSsdGNR/mHo7NaGBw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArshHp/LLM/blob/main/Embedding_Model_And_Retrieved_Chunks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0qHQ_DPsR_QD"
      },
      "outputs": [],
      "source": [
        "#New Embedding Model for Portuguese Language"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers"
      ],
      "metadata": {
        "id": "SAvrJU4vSAK2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "H1mRZ-NMSDuJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Instructor-XL model\n",
        "embedding_model = SentenceTransformer(\"hkunlp/instructor-xl\")\n"
      ],
      "metadata": {
        "id": "ZYj5G-usSJm9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define instruction + Portuguese sentence\n",
        "instruction = \"Represent this sentence for semantic similarity\"\n",
        "sentence_pt = \"O processo pode possuir mais de um localizador, mas nunca pode ficar sem um.\"\n",
        "\n",
        "# Combine instruction and input sentence\n",
        "input_text = [instruction, sentence_pt]\n",
        "\n",
        "#input_text = f\"{instruction}: {sentence_pt}\"  # Merge into one string\n",
        "\n",
        "# Get the embedding\n",
        "embedding = embedding_model.encode(input_text)"
      ],
      "metadata": {
        "id": "LgVdXc09SN3f"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print results\n",
        "print(f\"Embedding Shape: {embedding.shape}\")  # Expected: (1, 768)\n",
        "print(f\"First 5 embedding values: {embedding[1][:5]}\")  # Print first 5 dimensions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7oFH6doSQF0",
        "outputId": "3b597744-c30d-4cbb-bd16-e3ed11d8605e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Shape: (2, 768)\n",
            "First 5 embedding values: [ 0.02909243  0.00812454  0.01086273 -0.04523072 -0.06861272]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lets setup the ChromaDB to store the embeddings.**"
      ],
      "metadata": {
        "id": "2oW9tIeld4xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q chromadb"
      ],
      "metadata": {
        "id": "e6SF3MWHbsKz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Initialize the ChromaDB client\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")  # Stores embeddings persistently\n",
        "\n",
        "# Create or load a collection in ChromaDB\n",
        "collection = chroma_client.get_or_create_collection(name=\"ptbr_documents\")\n",
        "\n",
        "# Define instruction for embedding generation\n",
        "instruction = \"Represent this sentence for semantic similarity\""
      ],
      "metadata": {
        "id": "U9KxDHqhbfnz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Brazilian Portuguese document (replace with customer dataset here, manually crafted as sample)\n",
        "document_text = \"\"\"\n",
        "No eproc, o processo pode possuir mais de um localizador, mas nunca pode ficar sem um.\n",
        "√â importante lembrar que, no caso de m√∫ltiplos localizadores, o principal ficar√° em negrito quando expandido o campo para inclus√£o ou exclus√£o.\n",
        "Podemos pensar nos localizadores como etiquetas que s√£o coladas e retiradas do processo durante toda sua tramita√ß√£o.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "r4H0P9CycTzU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Split document into chunks #Doing basic splitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
        "chunks = text_splitter.split_text(document_text)\n",
        "\n",
        "# Step 2: Generate embeddings for each chunk\n",
        "chunk_embeddings = [embedding_model.encode(f\"{instruction}: {chunk}\") for chunk in chunks]\n",
        "\n",
        "# Step 3: Store embeddings in ChromaDB\n",
        "for i, chunk in enumerate(chunks):\n",
        "    collection.add(\n",
        "        ids=[f\"chunk_{i}\"],  # Unique ID for each chunk\n",
        "        documents=[chunk],  # Store the actual text\n",
        "        embeddings=[chunk_embeddings[i].tolist()]  # Convert NumPy array to list\n",
        "    )\n",
        "\n",
        "print(f\"‚úÖ Successfully stored {len(chunks)} chunks in ChromaDB!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOoYrp4RcXX2",
        "outputId": "af000019-ee23-4598-f1ba-d24fc9308604"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully stored 3 chunks in ChromaDB!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_text = \"Onde est√° o localizador no processo?\"\n",
        "query_embedding = embedding_model.encode(f\"{instruction}: {query_text}\").tolist()\n",
        "\n",
        "# Perform a similarity search in ChromaDB\n",
        "results = collection.query(\n",
        "    query_embeddings=[query_embedding],\n",
        "    n_results=3  # Retrieve top 3 matches\n",
        ")\n",
        "\n",
        "# Print the most relevant chunks\n",
        "print(\"üîç Top matches:\")\n",
        "for i, doc in enumerate(results[\"documents\"][0]):\n",
        "    print(f\"{i+1}. {doc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG9XoTT1ckrM",
        "outputId": "c8e27f21-6add-4a32-ae6e-9bcd8af68bfe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Top matches:\n",
            "1. Podemos pensar nos localizadores como etiquetas que s√£o coladas e retiradas do processo durante toda sua tramita√ß√£o.\n",
            "2. No eproc, o processo pode possuir mais de um localizador, mas nunca pode ficar sem um.\n",
            "3. √â importante lembrar que, no caso de m√∫ltiplos localizadores, o principal ficar√° em negrito quando expandido o campo para inclus√£o ou exclus√£o.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the above retrieved chunks to create the LLAMA 3.1 prompt and get the final response."
      ],
      "metadata": {
        "id": "58RPI40xdfmT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}