{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcYW11e12zZqPi8qzEuDCS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArshHp/LLM/blob/main/Multi_Turn_2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List of capabilities in this notebook\n",
        "\n",
        "1. Basic RAG pipeline with public test dataset\n",
        "2. CRAG using LLM chain filter & Rerankers\n",
        "3. Rephrase the RAG query based on previpus coversation for correct RAG retrievals\n",
        "4. Add Multi-tun Logic with Vector Based Embedding approach"
      ],
      "metadata": {
        "id": "OygwWjamiwCF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K6LbbsiUZBeH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain\n",
        "!pip install -q torch\n",
        "!pip install -q transformers\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q datasets\n",
        "!pip install -q faiss-cpu\n",
        "!pip install -q langchain_community"
      ],
      "metadata": {
        "id": "3pLZHCUOkhDn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain import HuggingFacePipeline"
      ],
      "metadata": {
        "id": "A_1kn239n1Kg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the pre-trained model you want to use\n",
        "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "\n",
        "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
        "model_kwargs = {'device':'cpu'}\n",
        "\n",
        "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=modelPath,     # Provide the pre-trained model's path\n",
        "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
        "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C58U8WSzn8mH",
        "outputId": "1302a514-0bcd-4b39-d08b-e205388cc92c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-a34a09b47e7d>:11: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"]=\"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\""
      ],
      "metadata": {
        "id": "mwhSYqujZ_mH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-groq"
      ],
      "metadata": {
        "id": "Px0AIbIqnwtY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\") #Replace with Finetune LLM model, its mainly for quick POC work\n"
      ],
      "metadata": {
        "id": "o3GzXORkaCb3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the external documents for Vector Database"
      ],
      "metadata": {
        "id": "J8VpjKhFhLw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-chroma"
      ],
      "metadata": {
        "id": "16Ypy4eIp5nm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma"
      ],
      "metadata": {
        "id": "vaEoPEfcqPm-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1oWBnoxBZ1Mpeond8XDUSO6J9oAjcRDyW"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_meoSze7mDcb",
        "outputId": "879e0332-b2b9-4b2e-9089-5629b027d292"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1oWBnoxBZ1Mpeond8XDUSO6J9oAjcRDyW\n",
            "From (redirected): https://drive.google.com/uc?id=1oWBnoxBZ1Mpeond8XDUSO6J9oAjcRDyW&confirm=t&uuid=f134eee4-c5ac-48a5-8e35-fbb1b8e7fa90\n",
            "To: /content/simplewiki-2020-11-01.jsonl.gz\n",
            "100% 50.2M/50.2M [00:00<00:00, 68.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import json\n",
        "\n",
        "wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
        "\n",
        "docs = []\n",
        "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
        "    for line in fIn:\n",
        "        data = json.loads(line.strip())\n",
        "\n",
        "        #Add all paragraphs\n",
        "        #passages.extend(data['paragraphs'])\n",
        "\n",
        "        #Only add the first paragraph\n",
        "        docs.append({\n",
        "                        'metadata': {\n",
        "                                        'title': data.get('title'),\n",
        "                                        'article_id': data.get('id')\n",
        "                        },\n",
        "                        'data': ' '.join(data.get('paragraphs')[0:3]) # restrict data to first 3 paragraphs to run later modules faster\n",
        "        })\n",
        "\n",
        "print(\"Passages:\", len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0emT28vmHSl",
        "outputId": "2882aa55-0132-4670-e8c5-f0d2b3ece0c8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passages: 169597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We subset our data so we only use a subset of wikipedia documents to run things faster\n",
        "docs = [doc for doc in docs for x in ['linguistics', 'india', 'cheetah']\n",
        "              if x in doc['data'].lower().split()]"
      ],
      "metadata": {
        "id": "o_HqXTa_mLlP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30LNNvaAmQ3N",
        "outputId": "43f32d41-cea0-48a5-c1f4-f150c5b82ecd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1364"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "docs = [Document(page_content=doc['data'],\n",
        "                 metadata=doc['metadata']) for doc in docs]"
      ],
      "metadata": {
        "id": "G3VeDnWfmTi5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7CAaP8WmWQ3",
        "outputId": "0e8f2af3-1f28-4454-a263-cbb6ecd3572d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1364"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=300)\n",
        "chunked_docs = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "P6t0bQGZmYbD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunked_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn7E2un3mahD",
        "outputId": "d60a45d5-3457-40da-c681-e65eb285a1f0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1388"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create vector DB of docs and embeddings - takes < 30s on Colab\n",
        "\n",
        "chroma_db = Chroma.from_documents(documents=chunked_docs,\n",
        "                                 collection_name='rag_db',\n",
        "                                  embedding=embeddings,\n",
        "                                  # need to set the distance function to cosine else it uses euclidean by default\n",
        "                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
        "                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                  persist_directory=\"./chroma_rag_db\")"
      ],
      "metadata": {
        "id": "x1LszxhblYKJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load from disk\n",
        "chroma_db = Chroma(persist_directory=\"./chroma_rag_db\",\n",
        "                   collection_name='rag_db',\n",
        "                   embedding_function=embeddings)"
      ],
      "metadata": {
        "id": "mcRY7b2Nmm65"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retriever alomg with LLM Chain FIlter & ReRankers"
      ],
      "metadata": {
        "id": "YN6MDskxheTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain.retrievers.document_compressors import LLMChainFilter\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "\n",
        "# Retriever 1 - simple cosine distance based retriever\n",
        "similarity_retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
        "                                              search_kwargs={\"k\": 5})\n",
        "\n",
        "#  decides which of the initially retrieved documents to filter out and which ones to return\n",
        "_filter = LLMChainFilter.from_llm(llm=llm)\n",
        "# Retriever 2 - retrieves the documents similar to query and then applies the filter\n",
        "compressor_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=_filter, base_retriever=similarity_retriever\n",
        ")\n",
        "\n",
        "# download an open-source reranker model - BAAI/bge-reranker-v2-m3\n",
        "reranker = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-large\")\n",
        "reranker_compressor = CrossEncoderReranker(model=reranker, top_n=3)\n",
        "# Retriever 3 - Uses a Reranker model to rerank retrieval results from the previous retriever\n",
        "final_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker_compressor, base_retriever=compressor_retriever\n",
        ")"
      ],
      "metadata": {
        "id": "uKVgdH76ndwz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets test the flow"
      ],
      "metadata": {
        "id": "hMfzvqCFhrDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is the old capital of India?\"\n",
        "docs = final_retriever.invoke(query)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqmcj4oJnqm6",
        "outputId": "865926b6-c288-464c-bc2a-811340291d2c"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='f1e05767-f10b-4b99-9caa-ab6e49b3bcc4', metadata={'article_id': '5117', 'title': 'New Delhi'}, page_content='New Delhi () is the capital of India and a union territory of the megacity of Delhi. It has a very old history and is home to several monuments where the city is expensive to live in. In traditional Indian geography it falls under the North Indian zone. The city has an area of about 42.7\\xa0km. New Delhi has a population of about 9.4 Million people.'),\n",
              " Document(id='0fca1e4f-89ba-4e19-9109-ec90e2dd6128', metadata={'article_id': '22106', 'title': 'Delhi'}, page_content='Delhi (; \"Dillī\"; \"Dillī\"; \"Dēhlī\"), officially the National Capital Territory of Delhi (NCT), is a territory in India. It includes the country\\'s capital New Delhi. It covers an area of . It is bigger than the Faroe Islands but smaller than Guadeloupe. Delhi is a part of the National Capital Region, which has 12.5 million residents. The governance of Delhi is like that of a state in India. It has its own legislature, high court and a council of executive ministers. Delhi is on the banks of the Yamuna River. Historians have evidence that people have been living in this region since at least the 6th century BC. People also believe that the legendary city of Indraprastha was here. This city has many remains and monuments of historic importance. The India Gate is a war memorial in Delhi. On the India gate there are names of some of the people who fought for India during 1914 until 1921.'),\n",
              " Document(id='995bffd4-e1be-4174-8bf8-29cb09488675', metadata={'article_id': '22106', 'title': 'Delhi'}, page_content='Delhi (; \"Dillī\"; \"Dillī\"; \"Dēhlī\"), officially the National Capital Territory of Delhi (NCT), is a territory in India. It includes the country\\'s capital New Delhi. It covers an area of . It is bigger than the Faroe Islands but smaller than Guadeloupe. Delhi is a part of the National Capital Region, which has 12.5 million residents. The governance of Delhi is like that of a state in India. It has its own legislature, high court and a council of executive ministers. Delhi is on the banks of the Yamuna River. Historians have evidence that people have been living in this region since at least the 6th century BC. People also believe that the legendary city of Indraprastha was here. This city has many remains and monuments of historic importance. The India Gate is a war memorial in Delhi. On the India gate there are names of some of the people who fought for India during 1914 until 1921.')]"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the LangChain along with Retrivers"
      ],
      "metadata": {
        "id": "9MBbjb6Nh2VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = \"\"\"You are an assistant for question-answering tasks.\n",
        "            Use the following pieces of retrieved context to answer the question.\n",
        "            If no context is present or if you don't know the answer, just say that you don't know.\n",
        "            Do not make up the answer unless it is there in the provided context.\n",
        "            Give a detailed answer with regard to the question.\n",
        "\n",
        "            Question:\n",
        "            {question}\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            Answer:\n",
        "         \"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(prompt)"
      ],
      "metadata": {
        "id": "5NlIuqnGn2kT"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYS_PROMPT = \"\"\" You are an AI assistant designed for high-quality question-answering. Your goal is to provide precise, well-structured, and contextually relevant responses based on the conversation history, context, and the user's query.\n",
        "\n",
        "## **Instructions for Response Generation:**\n",
        "1. **Understand Context:**\n",
        "   - Consider both the provided historical conversation messages and explicit context.\n",
        "   - Identify relevant information from the history that aligns with the user's latest question.\n",
        "   - Avoid redundant information if it has already been addressed.\n",
        "\n",
        "2. **Answer Clearly & Concisely:**\n",
        "   - Provide direct, accurate, and structured responses.\n",
        "   - When needed, break down complex answers into steps or bullet points.\n",
        "   - Use formal yet conversational language to ensure clarity.\n",
        "\n",
        "3. **Handle Ambiguity:**\n",
        "   - If the user's question is unclear, request clarification instead of making assumptions.\n",
        "   - If multiple interpretations exist, briefly list possible meanings and ask for confirmation.\n",
        "\n",
        "4. **Leverage Contextual Memory:**\n",
        "   - Use relevant prior exchanges to maintain continuity in the conversation.\n",
        "   - If a response depends on prior answers, ensure consistency while avoiding contradictions.\n",
        "\n",
        "5. **Adapt to User Intent & Preferences:**\n",
        "   - If the user has specific preferences (e.g., detailed explanations vs. concise summaries), align your response accordingly.\n",
        "   - Detect tone and formality based on prior interactions and adjust your response style.\n",
        "\n",
        "6. **Accuracy & Verification:**\n",
        "   - Base your responses on verifiable facts.\n",
        "   - If uncertain or if external verification is required, acknowledge the need for further confirmation.\n",
        "\n",
        "7. **Avoid Unnecessary Repetition:**\n",
        "   - Do not restate previous answers unless specifically requested or required for clarity.\n",
        "   - Summarize previous points efficiently when relevant.\n",
        "\n",
        "8. **Multi-Turn Conversations:**\n",
        "   - Keep track of unresolved queries and follow up if necessary.\n",
        "   - If a conversation thread requires closure, suggest the next steps.\n",
        "   \"\"\"\n"
      ],
      "metadata": {
        "id": "1EDUStd95Kuj"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import VectorStoreRetrieverMemory\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda"
      ],
      "metadata": {
        "id": "kPy_3JT75t9g"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define an improved ChatPromptTemplate with conditional handling\n",
        "prompt_template = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    You are an AI assistant that answers user questions based on provided context and conversation history.\n",
        "\n",
        "    Guidelines:\n",
        "    1. Use conversation history if relevant to maintain continuity.\n",
        "    2. If history is empty, rely only on the provided context.\n",
        "    3. If both history and context are empty, let the user know that you need more information.\n",
        "    4. Be precise, structured, and avoid repetition.\n",
        "\n",
        "    {% if history.strip() %}\n",
        "    Previous Conversation History:\n",
        "    {{ history }}\n",
        "    {% endif %}\n",
        "\n",
        "    {% if context.strip() %}\n",
        "    Retrieved Context:\n",
        "    {{ context }}\n",
        "    {% else %}\n",
        "    [Note: No additional context was provided.]\n",
        "    {% endif %}\n",
        "\n",
        "    User Question:\n",
        "    {{ question }}\n",
        "\n",
        "    Provide a clear and informative response.\n",
        "    \"\"\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "L5vp7m3S-y5G"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYS_PROMPT = \"\"\"Act as a helpful assistant and give brief answers\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# load 2 most similar conversation messages from vector db history for each new message \\ prompt\n",
        "# this uses cosine embedding similarity to load the top 2 similar messgages to the input prompt \\ query\n",
        "retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
        "                                   search_kwargs={\"k\": 5})\n",
        "memory = VectorStoreRetrieverMemory(retriever=retriever, return_messages=True)\n",
        "\n",
        "# creating our conversation chain now\n",
        "def get_memory_messages(query):\n",
        "    return [memory.load_memory_variables(query)['history']]\n",
        "\n",
        "conversation_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(get_memory_messages)\n",
        "    ) # sends current query (input by user at runtime) and history messages to next step\n",
        "      |\n",
        "    prompt # creates prompt using the previous two variables\n",
        "      |\n",
        "    llm # generates response using the prompt from previous step\n",
        ")\n"
      ],
      "metadata": {
        "id": "jlMat3SO3xNu"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Tell me about AI'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfc6xrquYwUp",
        "outputId": "86d857ee-5cf7-430f-f133-dcd8953ab0a7"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial Intelligence (AI) is a field of computer science that focuses on creating intelligent machines that can perform tasks that typically require human intelligence, such as learning, reasoning, problem-solving, and perception.\n",
            "\n",
            "**Types of AI:**\n",
            "\n",
            "1. **Narrow or Weak AI**: Designed to perform a specific task, such as facial recognition or language translation.\n",
            "2. **General or Strong AI**: A hypothetical AI system that can perform any intellectual task that a human can.\n",
            "3. **Superintelligence**: An AI system that is significantly more intelligent than the best human minds.\n",
            "\n",
            "**Applications of AI:**\n",
            "\n",
            "1. **Virtual assistants**: Siri, Alexa, and Google Assistant.\n",
            "2. **Image recognition**: Facebook's facial recognition feature.\n",
            "3. **Self-driving cars**: Companies like Tesla and Waymo are developing autonomous vehicles.\n",
            "4. **Healthcare**: AI is used to analyze medical images and diagnose diseases.\n",
            "5. **Customer service**: Chatbots are used to provide customer support.\n",
            "\n",
            "**Benefits of AI:**\n",
            "\n",
            "1. **Increased efficiency**: AI systems can automate tasks and free up human time.\n",
            "2. **Improved accuracy**: AI systems can reduce errors and improve decision-making.\n",
            "3. **Enhanced customer experience**: AI-powered chatbots and virtual assistants can provide 24/7 support.\n",
            "\n",
            "**Concerns about AI:**\n",
            "\n",
            "1. **Job displacement**: AI may automate jobs, leading to unemployment.\n",
            "2. **Bias and fairness**: AI systems can perpetuate biases and inequalities if they are trained on biased data.\n",
            "3. **Security and safety**: AI systems can be vulnerable to cyber attacks and may pose safety risks if they malfunction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What about deep learning'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKT0uZiNY1aC",
        "outputId": "5c576042-a7d7-4b24-a983-c5d88af16ba9"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Deep Learning** is a subfield of Machine Learning that focuses on the use of artificial neural networks with multiple layers to analyze and interpret data. These networks are designed to mimic the structure and function of the human brain, with each layer processing the input data in a more abstract and meaningful way.\n",
            "\n",
            "**Key Features of Deep Learning:**\n",
            "\n",
            "1. **Multiple Layers**: Deep learning models consist of multiple layers of artificial neurons, each of which processes the input data in a different way.\n",
            "2. **Hierarchical Representations**: Each layer in a deep learning model learns to represent the input data in a more abstract and meaningful way, allowing the model to capture complex patterns and relationships.\n",
            "3. **Non-Linear Transformations**: Deep learning models use non-linear transformations to map the input data to the output, allowing the model to learn complex and non-linear relationships.\n",
            "4. **Backpropagation**: Deep learning models use an algorithm called backpropagation to update the model's parameters and improve its performance.\n",
            "\n",
            "**Types of Deep Learning:**\n",
            "\n",
            "1. **Convolutional Neural Networks (CNNs)**: Designed for image and video processing, CNNs use convolutional and pooling layers to extract features from the input data.\n",
            "2. **Recurrent Neural Networks (RNNs)**: Designed for sequential data, RNNs use recurrent and feedback connections to process the input data over time.\n",
            "3. **Long Short-Term Memory (LSTM) Networks**: A type of RNN that uses memory cells to store information over long periods of time.\n",
            "4. **Generative Adversarial Networks (GANs)**: A type of deep learning model that uses two neural networks to generate new data that is similar to the input data.\n",
            "\n",
            "**Applications of Deep Learning:**\n",
            "\n",
            "1. **Computer Vision**: Deep learning models are widely used in computer vision applications, such as image classification, object detection, and image segmentation.\n",
            "2. **Natural Language Processing (NLP)**: Deep learning models are used in NLP applications, such as language translation, text classification, and sentiment analysis.\n",
            "3. **Speech Recognition**: Deep learning models are used in speech recognition applications, such as voice assistants and speech-to-text systems.\n",
            "4. **Robotics**: Deep learning models are used in robotics applications, such as object recognition, grasping, and manipulation.\n",
            "\n",
            "**Benefits of Deep Learning:**\n",
            "\n",
            "1. **Improved Accuracy**: Deep learning models can achieve state-of-the-art results in many applications, such as image classification and language translation.\n",
            "2. **Increased Efficiency**: Deep learning models can process large amounts of data quickly and efficiently, making them suitable for real-time applications.\n",
            "3. **Flexibility**: Deep learning models can be used in a wide range of applications, from computer vision to speech recognition.\n",
            "\n",
            "However, **Deep Learning also has some challenges**, such as:\n",
            "\n",
            "1. **Training Complexity**: Deep learning models require large amounts of data and computational resources to train, which can be a significant challenge.\n",
            "2. **Overfitting**: Deep learning models can overfit the training data, resulting in poor performance on new, unseen data.\n",
            "3. **Interpretability**: Deep learning models can be difficult to interpret, making it challenging to understand why the model is making certain predictions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.load_memory_variables({'query': 'What about machine learning?'})['history'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmUlbTj1Y7kq",
        "outputId": "58cf226b-d053-4e27-e947-afba553917e9"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computational Linguistics is a field of linguistics that deals with making computers understand human language. Some of the biggest sub-fields of computational linguistics are: Speech Recognition, which is a computer program that listens to people talk and writes down what they said Speech Synthesis, which is a computer program that takes writing and reads it out loud\n",
            "Computational Linguistics is a field of linguistics that deals with making computers understand human language. Some of the biggest sub-fields of computational linguistics are: Speech Recognition, which is a computer program that listens to people talk and writes down what they said Speech Synthesis, which is a computer program that takes writing and reads it out loud\n",
            "query: Tell me about AI\n",
            "output: Artificial Intelligence (AI) is a field of computer science that focuses on creating intelligent machines that can perform tasks that typically require human intelligence, such as:\n",
            "\n",
            "1. **Learning**: AI systems can learn from data and improve their performance over time.\n",
            "2. **Reasoning**: AI systems can make decisions based on their understanding of the world.\n",
            "3. **Problem-solving**: AI systems can solve complex problems, such as playing games or navigating through mazes.\n",
            "4. **Perception**: AI systems can understand and interpret sensory data, such as images and speech.\n",
            "\n",
            "There are several types of AI, including:\n",
            "\n",
            "1. **Narrow or Weak AI**: Designed to perform a specific task, such as facial recognition or language translation.\n",
            "2. **General or Strong AI**: A hypothetical AI system that can perform any intellectual task that a human can.\n",
            "3. **Superintelligence**: An AI system that is significantly more intelligent than the best human minds.\n",
            "\n",
            "Applications of AI include:\n",
            "\n",
            "1. **Virtual assistants**: Siri, Alexa, and Google Assistant.\n",
            "2. **Image recognition**: Facebook's facial recognition feature.\n",
            "3. **Self-driving cars**: Companies like Tesla and Waymo are developing autonomous vehicles.\n",
            "4. **Healthcare**: AI is used to analyze medical images and diagnose diseases.\n",
            "5. **Customer service**: Chatbots are used to provide customer support.\n",
            "\n",
            "The benefits of AI include:\n",
            "\n",
            "1. **Increased efficiency**: AI systems can automate tasks and free up human time.\n",
            "2. **Improved accuracy**: AI systems can reduce errors and improve decision-making.\n",
            "3. **Enhanced customer experience**: AI-powered chatbots and virtual assistants can provide 24/7 support.\n",
            "\n",
            "However, there are also concerns about the impact of AI on society, including:\n",
            "\n",
            "1. **Job displacement**: AI may automate jobs, leading to unemployment.\n",
            "2. **Bias and fairness**: AI systems can perpetuate biases and inequalities if they are trained on biased data.\n",
            "3. **Security and safety**: AI systems can be vulnerable to cyber attacks and may pose safety risks if they malfunction.\n",
            "\n",
            "Overall, AI has the potential to revolutionize many industries and improve people's lives. However, it's essential to consider the potential risks and ensure that AI is developed and used responsibly.\n",
            "query: What about deep learning\n",
            "output: **Deep Learning** is a subfield of Machine Learning that focuses on the use of artificial neural networks with multiple layers to analyze and interpret data. These networks are designed to mimic the structure and function of the human brain, with each layer processing the input data in a more abstract and meaningful way.\n",
            "\n",
            "**Key Features of Deep Learning:**\n",
            "\n",
            "1. **Multiple Layers**: Deep learning models consist of multiple layers of artificial neurons, each of which processes the input data in a different way.\n",
            "2. **Hierarchical Representations**: Each layer in a deep learning model learns to represent the input data in a more abstract and meaningful way, allowing the model to capture complex patterns and relationships.\n",
            "3. **Non-Linear Transformations**: Deep learning models use non-linear transformations to map the input data to the output, allowing the model to learn complex and non-linear relationships.\n",
            "4. **Backpropagation**: Deep learning models use an algorithm called backpropagation to update the model's parameters and improve its performance.\n",
            "\n",
            "**Types of Deep Learning:**\n",
            "\n",
            "1. **Convolutional Neural Networks (CNNs)**: Designed for image and video processing, CNNs use convolutional and pooling layers to extract features from the input data.\n",
            "2. **Recurrent Neural Networks (RNNs)**: Designed for sequential data, RNNs use recurrent and feedback connections to process the input data over time.\n",
            "3. **Long Short-Term Memory (LSTM) Networks**: A type of RNN that uses memory cells to store information over long periods of time.\n",
            "4. **Generative Adversarial Networks (GANs)**: A type of deep learning model that uses two neural networks to generate new data that is similar to the input data.\n",
            "\n",
            "**Applications of Deep Learning:**\n",
            "\n",
            "1. **Computer Vision**: Deep learning models are widely used in computer vision applications, such as image classification, object detection, and image segmentation.\n",
            "2. **Natural Language Processing (NLP)**: Deep learning models are used in NLP applications, such as language translation, text classification, and sentiment analysis.\n",
            "3. **Speech Recognition**: Deep learning models are used in speech recognition applications, such as voice assistants and speech-to-text systems.\n",
            "4. **Robotics**: Deep learning models are used in robotics applications, such as object recognition, grasping, and manipulation.\n",
            "\n",
            "**Benefits of Deep Learning:**\n",
            "\n",
            "1. **Improved Accuracy**: Deep learning models can achieve state-of-the-art results in many applications, such as image classification and language translation.\n",
            "2. **Increased Efficiency**: Deep learning models can process large amounts of data quickly and efficiently, making them suitable for real-time applications.\n",
            "3. **Flexibility**: Deep learning models can be used in a wide range of applications, from computer vision to speech recognition.\n",
            "\n",
            "However, **Deep Learning also has some challenges**, such as:\n",
            "\n",
            "1. **Training Complexity**: Deep learning models require large amounts of data and computational resources to train, which can be a significant challenge.\n",
            "2. **Overfitting**: Deep learning models can overfit the training data, resulting in poor performance on new, unseen data.\n",
            "3. **Interpretability**: Deep learning models can be difficult to interpret, making it challenging to understand why the model is making certain predictions.\n",
            "Dabbala Rajagopal (Raj) Reddy (born Kattoor, India 19 June 1937) is an Indian computer scientist and Carnegie Mellon University professor. He is noted for his contributions to artificial intelligence, robotics and speech recognition. Reddy developed Hearsay I, the first speech recognition system, which is the foundation for all commercial speech recognition programs today. He received his Master’s degree in computer science at the University of New South Wales in 1961 and his PhD from Stanford University in 1966. Reddy served on the faculty of Stanford and Carnegie Mellon University for over 40 years. He has won many international awards for his work, including the Turing Award (which is computer science’s greatest honor), membership in the National Academy of Engineering and the American Academy of Arts and Sciences, the Legion of Honour, the IBM Research Ralph Gomory Fellow Award, the Padma Bhushan, the Okawa Prize, the Honda Prize, the IJCAI Donald E. Walker Distinguished Service Award, and the Vannevar Bush Award.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = \"\"\"You are an assistant for question-answering tasks.\n",
        "            Use the following pieces of retrieved context to answer the question.\n",
        "            If no context is present or if you don't know the answer, just say that you don't know.\n",
        "            Do not make up the answer unless it is there in the provided context.\n",
        "            Give a detailed answer with regard to the question.\n",
        "\n",
        "\n",
        "            Question:\n",
        "            {question}\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            Answer:\n",
        "         \"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(prompt)"
      ],
      "metadata": {
        "id": "ZHFQv9lVaOvF"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "qa_rag_chain = (\n",
        "    {\n",
        "        \"context\": (final_retriever\n",
        "                      |\n",
        "                    format_docs),\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "      |\n",
        "    prompt_template\n",
        "      |\n",
        "    llm\n",
        ")\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "query = \"What is the financial capital of India?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "id": "oHkQv0asoBHl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "7c257515-ed04-459c-812b-0165056cd800"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the provided context, I don't know what the financial capital of India is. The context only mentions New Delhi as the capital of India and a union territory of the megacity of Delhi, but it doesn't provide any information related to the financial capital of India."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yPfZxsl0aiFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We need to merge Question, Answers , Message history into the same chain\n"
      ],
      "metadata": {
        "id": "84OLx48MaZgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_memory_messages(query):\n",
        "    # Pass query as a dictionary with the expected key ('query')\n",
        "    return [memory.load_memory_variables({\"query\": query})['history']]"
      ],
      "metadata": {
        "id": "xofldhuGAwSK"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = \"\"\"You are an assistant for question-answering tasks.\n",
        "            Use the following pieces of retrieved context to answer the question.\n",
        "            If no context is present or if you don't know the answer, just say that you don't know.\n",
        "            Do not make up the answer unless it is there in the provided context.\n",
        "            Give a detailed answer with regard to the question.\n",
        "            Guidelines:\n",
        "                1. Use conversation history if relevant to maintain continuity.\n",
        "                2. If history is empty, rely only on the provided context.\n",
        "                3. If both history and context are empty, let the user know that you need more information.\n",
        "                4. Be precise, structured, and avoid repetition.\n",
        "\n",
        "            Question:\n",
        "            {question}\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            History:\n",
        "            {history}\n",
        "\n",
        "            Answer:\n",
        "         \"\"\"\n",
        "\n",
        "merge_prompt_template = ChatPromptTemplate.from_template(prompt)"
      ],
      "metadata": {
        "id": "eyIZsq8oaYwH"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_chain = (\n",
        "    {\n",
        "        \"history\": RunnableLambda(get_memory_messages),\n",
        "        \"context\": (final_retriever | format_docs),\n",
        "        \"question\": RunnablePassthrough(), # Extract the question from the input directly\n",
        "\n",
        "    }\n",
        "    |\n",
        "    merge_prompt_template\n",
        "    |\n",
        "    llm\n",
        ")\n"
      ],
      "metadata": {
        "id": "AqxzkuH-ezG2"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the financial capital of India?\"\n",
        "result = merged_chain.invoke(query) # Invoke with the query\n",
        "memory.save_context({\"query\": query}, {\"output\": result.content}) # remember to save your current conversation in memory\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "m1XMk3Iw5yjd",
        "outputId": "6e457b5c-f3d3-4959-d52a-a31a7c3dc149"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the provided context, I do not have enough information to determine the financial capital of India. However, I can infer that the context does not mention the financial capital of India explicitly.\n\nHowever, since the question is about the financial capital of India, it might be related to the stock exchange in India. The context does mention the National Stock Exchange of India Limited (NSE), which is based in Mumbai. Mumbai is often considered the financial capital of India due to the presence of the Bombay Stock Exchange (BSE) and the National Stock Exchange of India (NSE), which are two of the largest stock exchanges in India.\n\nTo answer your question more accurately, I would say that Mumbai is often considered the financial capital of India. However, please note that this is an inference based on the context provided, and I do not have explicit information about the financial capital of India."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the capitol of India?\"\n",
        "result = merged_chain.invoke(query) # Invoke with the query\n",
        "memory.save_context({\"query\": query}, {\"output\": result.content}) # remember to save your current conversation in memory\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "75D9YXWUbXwb",
        "outputId": "2c1cc8dc-3f02-4470-c200-e07399bfb13e"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The capital of India is New Delhi. This information is present in the context provided, specifically in the paragraphs that describe New Delhi as the capital of India and a union territory of the megacity of Delhi."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Rephrase Logic, if ambigous"
      ],
      "metadata": {
        "id": "Mg5tcC9riCAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "\n",
        "\n",
        "\n",
        "def rewrite_query(current_query, previous_query=None, previous_answer=None, domain=\"General\"):\n",
        "    \"\"\"Uses LLM model to rewrite a user's query, considering past queries and answers for context resolution.\"\"\"\n",
        "\n",
        "    system_prompt = \"\"\"You are an intelligent AI assistant that refines user queries for a Retrieval-Augmented Generation (RAG) system.\n",
        "    - Improve clarity, completeness, and domain specificity.\n",
        "    - If the previous query and its answer provide context, use them to resolve ambiguity.\n",
        "    - Preserve the original intent while making the query more effective for retrieval.\n",
        "\n",
        "    Return only the rewritten query, without extra text.\n",
        "    \"\"\"\n",
        "\n",
        "    human_prompt = f\"User's new query: {current_query}\\n\"\n",
        "\n",
        "    if previous_query:\n",
        "        human_prompt += f\"Previous query: {previous_query}\\n\"\n",
        "\n",
        "    if previous_answer:\n",
        "        human_prompt += f\"Previous answer: {previous_answer}\\n\"\n",
        "\n",
        "    human_prompt += f\"Domain: {domain}\\n\\nRewrite the query.\"\n",
        "\n",
        "    # Call LLM to refine the query\n",
        "    response = llm([SystemMessage(content=system_prompt), HumanMessage(content=human_prompt)])\n",
        "    print (\"LLM Response after query rewrite:\", response.content.strip())\n",
        "    return response.content.strip()\n"
      ],
      "metadata": {
        "id": "HLxtI9PWt6aG"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def is_ambiguous_query_rule_based(query):\n",
        "    \"\"\"Returns True if the query is ambiguous based on simple rules.\"\"\"\n",
        "\n",
        "    # Short queries with no context\n",
        "    if len(query.split()) <= 3:\n",
        "        return True\n",
        "\n",
        "    # Follow-up words indicating implicit reference  #Use the local language words if there is no translation service\n",
        "    follow_up_words = [\"this\", \"that\", \"it\", \"one\", \"those\", \"these\", \"what about\", \"can I use it\"]\n",
        "    if any(word in query.lower() for word in follow_up_words):\n",
        "        return True\n",
        "\n",
        "    # Generic queries that need expansion #Use the local language words if there is no translation service\n",
        "    vague_patterns = [\n",
        "        r\"\\bhow\\b\", r\"\\bwhy\\b\", r\"\\bwhat about\\b\", r\"\\bdoes it\\b\", r\"\\bcan it\\b\",\n",
        "        r\"\\bis it\\b\", r\"\\bscaling\\b\", r\"\\bperformance\\b\"\n",
        "    ]\n",
        "    if any(re.search(pattern, query.lower()) for pattern in vague_patterns):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def is_ambiguous_query_llm(query):\n",
        "    \"\"\"Uses LLM to detect if a query needs rewriting.\"\"\"\n",
        "    system_prompt = \"\"\"You are an AI assistant that detects ambiguous queries.\n",
        "    If a query is vague, implicit, or requires prior context, return 'Yes'.\n",
        "    Otherwise, return 'No'.\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm([\n",
        "        SystemMessage(content=system_prompt),\n",
        "        HumanMessage(content=f\"Query: {query}\\n\\nIs this query ambiguous? (Yes/No)\")\n",
        "    ])\n",
        "\n",
        "    return response.content.strip().lower() == \"yes\"\n"
      ],
      "metadata": {
        "id": "l3GU74IQuBlJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rewrite_query_if_needed(current_query, previous_query=None, previous_answer=None, domain=\"General\", use_llm_check=False):\n",
        "    \"\"\"Decides whether to rewrite the query based on ambiguity detection.\"\"\"\n",
        "\n",
        "    # First, use rule-based detection\n",
        "    if not is_ambiguous_query_rule_based(current_query):\n",
        "        print (\"Rule based says, it's not ambiguous\")\n",
        "        return current_query  # Return original if clear\n",
        "\n",
        "    # Optional: Use LLM for ambiguity check\n",
        "    if use_llm_check and not is_ambiguous_query_llm(current_query):\n",
        "        print (\"LLM based says, it's not ambiguous\")\n",
        "        return current_query  # LLM says it's not ambiguous\n",
        "\n",
        "    # If ambiguous, rewrite the query\n",
        "    #print (\"Calling Rewriting Module...\")\n",
        "    return rewrite_query(current_query, previous_query, previous_answer, domain)"
      ],
      "metadata": {
        "id": "TxW4EiB2uH_l"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RAG_pipeline(user_query, conversation_history, use_llm_check=False):\n",
        "    \"\"\"Handles full RAG flow: query rewriting → retrieval → response generation.\"\"\"\n",
        "\n",
        "    # Retrieve previous query & answer from history\n",
        "    previous_query = conversation_history[-1]['query'] if conversation_history else None\n",
        "    previous_answer = conversation_history[-1]['answer'] if conversation_history else None\n",
        "\n",
        "    # Rewrite the query only if necessary\n",
        "    rewritten_query = rewrite_query_if_needed(user_query, previous_query, previous_answer, use_llm_check=use_llm_check)\n",
        "\n",
        "    # Retrieve documents\n",
        "    result = qa_rag_chain.invoke(rewritten_query)\n",
        "\n",
        "    # Store query & answer in history\n",
        "    conversation_history.append({\"query\": rewritten_query, \"answer\": result.content})\n",
        "    return result.content\n"
      ],
      "metadata": {
        "id": "2Tw6uarIpBmF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets test the new rephrase logic"
      ],
      "metadata": {
        "id": "vh-8fpFKiNYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_history = []\n",
        "\n",
        "# Example: Well-formed query (no rewriting needed)\n",
        "query1 = \"What is locator?\"\n",
        "response1 = RAG_pipeline(query1, conversation_history)\n",
        "print(\"\\n🤖 AI Response 1:\\n\\n\", display(Markdown(response1)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "gZ-ylFSGpvXp",
        "outputId": "889c40ef-3736-44c1-be7e-7e38cab0ae55"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling Rewriting Module...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-692ce8a034a5>:27: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = llm([SystemMessage(content=system_prompt), HumanMessage(content=human_prompt)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Response after query rewrite: What is the definition of a locator in general terms.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on general knowledge, I found that a locator is a term used in various fields, including geography, computer science, and navigation.\n\nIn general terms, a locator is an item or device that helps to find or identify a specific location, object, or position. It provides information or a means to determine the exact position or location of something.\n\nIn geography, a locator can refer to a geographical feature such as a mountain, river, or landmark that helps to identify a location.\n\nIn computer science, a locator can be a software component or algorithm that helps to locate an object, user, or device within a network or system. Examples of locators in computer science include IP addresses, port numbers, and DNS (Domain Name System) servers.\n\nIn navigation, a locator can be a device or instrument that helps to determine one's position or location, such as a compass, GPS (Global Positioning System) device, or a map.\n\nIn general, a locator serves as a reference point or a means to identify a location or position, making it easier to navigate, communicate, or interact with the world around us."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🤖 AI Response 1:\n",
            "\n",
            " None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"How to automate them?\"\n",
        "response1 = RAG_pipeline(query1, conversation_history)\n",
        "print(\"\\n🤖 AI Response 1:\\n\", display(Markdown(response1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "XfWsaKoq1IQs",
        "outputId": "0f8b9481-4a69-49ca-cc92-e704c8955935"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling Rewriting Module...\n",
            "LLM Response after query rewrite: How can I automate the process of finding or identifying a specific location, object, or position using locators in computer science or navigation?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the provided context, I don't see any information that directly answers your question. However, I can provide a general explanation on how to automate the process of finding or identifying a specific location, object, or position using locators in computer science or navigation.\n\nIn computer science, locators are used to identify or find specific elements or objects within a system. This can be achieved through various techniques and tools, including:\n\n1. **Object Detection**: This involves using computer vision and machine learning algorithms to detect and identify specific objects within an image or video. This can be used in applications such as self-driving cars, surveillance systems, and robotics.\n2. **Geolocation**: This involves using GPS, Wi-Fi, and other location-based services to determine the location of a device or object. This can be used in applications such as ride-hailing services, navigation systems, and location-based advertising.\n3. **HTML/CSS Selectors**: These are used to locate specific elements within an HTML document, such as buttons, forms, and links. This is commonly used in web automation and web scraping applications.\n4. **Mouse and Keyboard Events**: These can be used to simulate user interactions with a system, such as clicking on specific buttons or typing in specific fields.\n\nIn navigation, locators can be used to identify specific locations, such as:\n\n1. **GPS Coordinates**: These can be used to identify the location of a device or object on a map.\n2. **Address Matching**: This involves using algorithms to match a given address with a specific location on a map.\n3. **Geocoding**: This involves converting a human-readable address into a set of GPS coordinates.\n\nTo automate the process of finding or identifying a specific location, object, or position using locators, you can use various tools and techniques, including:\n\n1. **Programming languages**: Such as Python, Java, and C++, which can be used to write scripts and programs that interact with systems and perform tasks such as object detection and geolocation.\n2. **APIs and libraries**: Such as OpenCV, TensorFlow, and Google Maps API, which can be used to access and manipulate data related to object detection, geolocation, and navigation.\n3. **Automation frameworks**: Such as Selenium, Appium, and Robot Framework, which can be used to automate interactions with systems and perform tasks such as web scraping and navigation.\n\nI hope this provides a general explanation of how to automate the process of finding or identifying a specific location, object, or position using locators in computer science or navigation."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🤖 AI Response 1:\n",
            " None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#New Example\n",
        "conversation_history = []\n",
        "\n",
        "# Example: Well-formed query (no rewriting needed)\n",
        "query1 = \"What is the finanacial capital of India?\"\n",
        "response1 = RAG_pipeline(query1, conversation_history)\n",
        "print(\"\\n🤖 AI Response 1:\\n\", display(Markdown(response1)))\n",
        "\n",
        "query2 = \"How to go there from Bangalore?\"\n",
        "response2 = RAG_pipeline(query2, conversation_history,use_llm_check=True)\n",
        "print(\"\\n🤖 AI Response 2:\\n\", display(Markdown(response2)))\n",
        "\n",
        "query3 = \"distance between both\"\n",
        "response3 = RAG_pipeline(query3, conversation_history,use_llm_check=True)\n",
        "print(\"\\n🤖 AI Response 3:\\n\", display(Markdown(response3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "id": "SIAEE4aV2uUJ",
        "outputId": "879c81f8-51cb-491a-d026-edc3f747933c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling Rewriting Module...\n",
            "LLM Response after query rewrite: What is the financial capital of India?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I don't know the financial capital of India based on the provided context. The context does mention the capital of India as New Delhi and provides general information about Delhi, but it does not mention the financial capital."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🤖 AI Response 1:\n",
            " None\n",
            "Calling Rewriting Module...\n",
            "LLM Response after query rewrite: What is the financial capital of India and how to reach it from Bangalore?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The financial capital of India is Mumbai. \n\nTo reach Mumbai from Bangalore, there are several options:\n\n1. **By Air**: The fastest way to reach Mumbai from Bangalore is by taking a flight. The Bengaluru International Airport (BLR) is the nearest airport to Bangalore, and the Chhatrapati Shivaji Maharaj International Airport (BOM) is the nearest airport to Mumbai. You can take a flight from BLR to BOM, which takes approximately 1.5 hours. From the airport, you can take a taxi or a metro to reach your destination.\n\n2. **By Train**: Another option to reach Mumbai from Bangalore is by taking a train. The Yesvantpur Junction (YPR) is the nearest railway station to Bangalore, and the Mumbai Central (BCT) is the nearest railway station to Mumbai. You can take a train from YPR to BCT, which takes approximately 14-16 hours. You can book your ticket in advance on the Indian Railways website or at the railway station.\n\n3. **By Bus**: You can also reach Mumbai from Bangalore by taking a bus. The BMTC (Bangalore Metropolitan Transport Corporation) operates bus services from Bangalore to Mumbai. The journey takes approximately 24-30 hours, depending on the type of bus and the route taken.\n\n4. **By Car**: If you prefer to drive, you can take a car from Bangalore to Mumbai. The journey takes approximately 14-16 hours, depending on the traffic and the route taken. You can take the National Highway 48 (NH 48) from Bangalore to Mumbai.\n\nIt's worth noting that the best option for you will depend on your budget, time constraints, and personal preferences."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🤖 AI Response 2:\n",
            " None\n",
            "Calling Rewriting Module...\n",
            "LLM Response after query rewrite: What are the distances between Bangalore and Mumbai by air, train, bus, and car?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Unfortunately, the provided context is empty. However, I can provide the typical distances between Bangalore and Mumbai by different modes of transport.\n\nThe distance between Bangalore and Mumbai is approximately 765 kilometers (475 miles). \n\n1. **Air:** The distance by air is not a physical distance but rather the flight duration, which is approximately 1 hour and 20 minutes to 1 hour and 30 minutes.\n \n2. **Train:** The distance by train varies depending on the route taken. However, the approximate distance by train is also around 765 kilometers. \n\n3. **Bus:** The distance by bus is also around 765 kilometers. The exact distance may vary depending on the bus route taken.\n\n4. **Car:** The distance by car is approximately 765 kilometers. The driving duration is around 12-14 hours, depending on traffic conditions and the route taken.\n\nPlease note that these distances are approximate and may vary depending on the specific route taken and mode of transport."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🤖 AI Response 3:\n",
            " None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UohUUtq13MuQ",
        "outputId": "f90e0f8d-275a-448f-bec2-6079e6d933b2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'query': 'What is the financial capital of India?',\n",
              "  'answer': \"I don't know the financial capital of India based on the provided context. The context does mention the capital of India as New Delhi and provides general information about Delhi, but it does not mention the financial capital.\"},\n",
              " {'query': 'What is the financial capital of India and how to reach it from Bangalore?',\n",
              "  'answer': \"The financial capital of India is Mumbai. \\n\\nTo reach Mumbai from Bangalore, there are several options:\\n\\n1. **By Air**: The fastest way to reach Mumbai from Bangalore is by taking a flight. The Bengaluru International Airport (BLR) is the nearest airport to Bangalore, and the Chhatrapati Shivaji Maharaj International Airport (BOM) is the nearest airport to Mumbai. You can take a flight from BLR to BOM, which takes approximately 1.5 hours. From the airport, you can take a taxi or a metro to reach your destination.\\n\\n2. **By Train**: Another option to reach Mumbai from Bangalore is by taking a train. The Yesvantpur Junction (YPR) is the nearest railway station to Bangalore, and the Mumbai Central (BCT) is the nearest railway station to Mumbai. You can take a train from YPR to BCT, which takes approximately 14-16 hours. You can book your ticket in advance on the Indian Railways website or at the railway station.\\n\\n3. **By Bus**: You can also reach Mumbai from Bangalore by taking a bus. The BMTC (Bangalore Metropolitan Transport Corporation) operates bus services from Bangalore to Mumbai. The journey takes approximately 24-30 hours, depending on the type of bus and the route taken.\\n\\n4. **By Car**: If you prefer to drive, you can take a car from Bangalore to Mumbai. The journey takes approximately 14-16 hours, depending on the traffic and the route taken. You can take the National Highway 48 (NH 48) from Bangalore to Mumbai.\\n\\nIt's worth noting that the best option for you will depend on your budget, time constraints, and personal preferences.\"},\n",
              " {'query': 'What are the distances between Bangalore and Mumbai by air, train, bus, and car?',\n",
              "  'answer': 'Unfortunately, the provided context is empty. However, I can provide the typical distances between Bangalore and Mumbai by different modes of transport.\\n\\nThe distance between Bangalore and Mumbai is approximately 765 kilometers (475 miles). \\n\\n1. **Air:** The distance by air is not a physical distance but rather the flight duration, which is approximately 1 hour and 20 minutes to 1 hour and 30 minutes.\\n \\n2. **Train:** The distance by train varies depending on the route taken. However, the approximate distance by train is also around 765 kilometers. \\n\\n3. **Bus:** The distance by bus is also around 765 kilometers. The exact distance may vary depending on the bus route taken.\\n\\n4. **Car:** The distance by car is approximately 765 kilometers. The driving duration is around 12-14 hours, depending on traffic conditions and the route taken.\\n\\nPlease note that these distances are approximate and may vary depending on the specific route taken and mode of transport.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query='distance between both'\n",
        "previous_query='What is the best route or mode of transportation from Bangalore to Mumbai?'\n",
        "previous_answer=\"\"\"transportation options from Bangalore to Mumbai.\\n\\nTypically, the most convenient and efficient way to travel from Bangalore to Mumbai is by flight, which takes approximately 1.5 hours. You can fly from Kempegowda International Airport (BLR) in Bangalore to Chhatrapati Shivaji Maharaj International Airport (BOM) in Mumbai.\"\"\"\n",
        "rewritten_query = rewrite_query_if_needed(user_query, previous_query, previous_answer, use_llm_check=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85gmwN0g5f2G",
        "outputId": "781915ce-5408-4ff7-bf3b-172018aa2603"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling Rewriting Module...\n",
            "LLM Response after query rewrite: What are the distances between Kempegowda International Airport in Bangalore and Chhatrapati Shivaji Maharaj International Airport in Mumbai?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick testing only snippets"
      ],
      "metadata": {
        "id": "pZz_B9X4iaIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ret = is_ambiguous_query_rule_based(\"Hello, I want to know about something\")\n",
        "print (ret)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuwLmKNny_U7",
        "outputId": "8a9e248d-bb67-48c9-c17b-ad091127c312"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ret2 = is_ambiguous_query_llm(\"Hello, I want to know about something\")\n",
        "print (ret2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c64CXw2-2AqT",
        "outputId": "444bff99-015b-4a95-cab1-2e7806415c74"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query='How to automate them ?'\n",
        "previous_query='What are locators?'\n",
        "previous_answer=\"\"\"A general answer to know about locators.\"\"\"\n",
        "rewritten_query = rewrite_query_if_needed(user_query, previous_query, previous_answer, use_llm_check=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxToqMZp6Nq2",
        "outputId": "76d8320d-66b8-47e4-f4b1-a438c105d66c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling Rewriting Module...\n",
            "LLM Response after query rewrite: Automating what specific process or tasks related to locators?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (rewritten_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aQgliD86XZy",
        "outputId": "4393f91e-e85e-4968-fed9-c9675aa32dce"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automating what specific process or tasks related to locators?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Multi-turn Logic alog with repharse ambigous user queries"
      ],
      "metadata": {
        "id": "sSXr_-rEm8HX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Rephrase_Multiturn_RAG_pipeline(user_query, conversation_history, use_llm_check=False):\n",
        "    \"\"\"Handles full RAG flow: query rewriting → retrieval → response generation.\"\"\"\n",
        "\n",
        "    # Retrieve previous query & answer from history\n",
        "    previous_query = conversation_history[-1]['query'] if conversation_history else None\n",
        "    previous_answer = conversation_history[-1]['answer'] if conversation_history else None\n",
        "\n",
        "    # Rewrite the query only if necessary\n",
        "    rewritten_query = rewrite_query_if_needed(user_query, previous_query, previous_answer, use_llm_check=use_llm_check)\n",
        "\n",
        "    # Retrieve documents\n",
        "    result = merged_chain.invoke(rewritten_query)\n",
        "    memory.save_context({\"query\": query}, {\"output\": result.content}) # remember to save your current conversation in memory\n",
        "\n",
        "    # Store query & answer in history\n",
        "    conversation_history.append({\"query\": rewritten_query, \"answer\": result.content})\n",
        "    return result.content\n"
      ],
      "metadata": {
        "id": "ZDJksgi7DlAj"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#New Example\n",
        "conversation_history = []\n",
        "\n",
        "# Example: Well-formed query (no rewriting needed)\n",
        "query1 = \"What is the finanacial capital of India?\"\n",
        "response1 = Rephrase_Multiturn_RAG_pipeline(query1, conversation_history)\n",
        "#print(\"\\n🤖 AI Response 1:\\n\", display(Markdown(response1)))\n",
        "\n",
        "query2 = \"How to go there from Bangalore?\"\n",
        "response2 = Rephrase_Multiturn_RAG_pipeline(query2, conversation_history,use_llm_check=True)\n",
        "#print(\"\\n🤖 AI Response 2:\\n\", display(Markdown(response2)))\n",
        "\n",
        "query3 = \"distance between both\"\n",
        "response3 = Rephrase_Multiturn_RAG_pipeline(query3, conversation_history,use_llm_check=True)\n",
        "#print(\"\\n🤖 AI Response 3:\\n\", display(Markdown(response3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNriiNCDisO2",
        "outputId": "e35dc639-26f7-4bb7-e6c5-d98397e09423"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Response after query rewrite: What is the financial capital of India?\n",
            "LLM Response after query rewrite: What is the best route to travel from Bangalore to Mumbai?\n",
            "LLM Response after query rewrite: What is the approximate road distance between Bangalore and Mumbai?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Well-formed query (no rewriting needed)\n",
        "#New Example\n",
        "conversation_history = []\n",
        "\n",
        "query1 = \"What is the finanacial capital of India?\"\n",
        "response1 = Rephrase_Multiturn_RAG_pipeline(query1, conversation_history)\n",
        "#print(\"\\n🤖 AI Response 1:\\n\", display(Markdown(response1)))\n",
        "\n",
        "query2 = \"How to go there from Bangalore?\"\n",
        "response2 = Rephrase_Multiturn_RAG_pipeline(query2, conversation_history,use_llm_check=False)\n",
        "#print(\"\\n🤖 AI Response 2:\\n\", display(Markdown(response2)))\n",
        "\n",
        "query3 = \"distance between both\"\n",
        "response3 = Rephrase_Multiturn_RAG_pipeline(query3, conversation_history,use_llm_check=False)\n",
        "#print(\"\\n🤖 AI Response 3:\\n\", display(Markdown(response3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oy7Y2fMeJBSG",
        "outputId": "ba2f1a23-e637-4f68-f9d5-9067318c846c"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Response after query rewrite: What is the financial capital of India with a population of over 1.3 billion people?\n",
            "LLM Response after query rewrite: What are the directions from Bangalore to Mumbai?\n",
            "LLM Response after query rewrite: What is the approximate road distance and flight duration between Bangalore and Mumbai?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k_QOw9h8JiNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#New Example\n",
        "conversation_history = []\n",
        "\n",
        "# Example: Well-formed query (no rewriting needed)\n",
        "query1 = \"What is locators\"\n",
        "response1 = Rephrase_Multiturn_RAG_pipeline(query1, conversation_history,use_llm_check=False)\n",
        "#print(\"\\n🤖 AI Response 1:\\n\", display(Markdown(response1)))\n",
        "\n",
        "query2 = \"How to automate them\"\n",
        "response2 = Rephrase_Multiturn_RAG_pipeline(query2, conversation_history,use_llm_check=False)\n",
        "#print(\"\\n🤖 AI Response 2:\\n\", display(Markdown(response2)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "fhqCSz5YHcct",
        "outputId": "e7d11b6f-2127-4aeb-f4b3-c69553e0c0c5"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Response after query rewrite: What do you mean by \"locators\", specifically in what context (e.g. computing, geography, etc.)?\n",
            "LLM Response after query rewrite: Given the context from the previous conversation, I'm assuming you're looking to automate UI interactions. Can you provide more information on what you're trying to automate, specifically in terms of software testing or UI automation?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j52rz6n7emgtktsg2hy04h2x` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 5814, Requested 675. Please try again in 4.888s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-215-2ba01e41439a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mquery2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"How to automate them\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresponse2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRephrase_Multiturn_RAG_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversation_history\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_llm_check\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#print(\"\\n🤖 AI Response 2:\\n\", display(Markdown(response2)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-203-820fca1646b2>\u001b[0m in \u001b[0;36mRephrase_Multiturn_RAG_pipeline\u001b[0;34m(user_query, conversation_history, use_llm_check)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Retrieve documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewritten_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"output\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# remember to save your current conversation in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3022\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3023\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3024\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3726\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3727\u001b[0m                 ]\n\u001b[0;32m-> 3728\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3729\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3730\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3726\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3727\u001b[0m                 ]\n\u001b[0;32m-> 3728\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3729\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3730\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_invoke_step\u001b[0;34m(step, input, config, key)\u001b[0m\n\u001b[1;32m   3710\u001b[0m             )\n\u001b[1;32m   3711\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3712\u001b[0;31m                 return context.run(\n\u001b[0m\u001b[1;32m   3713\u001b[0m                     \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3714\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3022\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3023\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3024\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/retrievers.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0m_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expects_other_args\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_arg_supported\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 result = self._get_relevant_documents(\n\u001b[0m\u001b[1;32m    260\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/retrievers/contextual_compression.py\u001b[0m in \u001b[0;36m_get_relevant_documents\u001b[0;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mSequence\u001b[0m \u001b[0mof\u001b[0m \u001b[0mrelevant\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \"\"\"\n\u001b[0;32m---> 44\u001b[0;31m         docs = self.base_retriever.invoke(\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/retrievers.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0m_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expects_other_args\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_arg_supported\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 result = self._get_relevant_documents(\n\u001b[0m\u001b[1;32m    260\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/retrievers/contextual_compression.py\u001b[0m in \u001b[0;36m_get_relevant_documents\u001b[0;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         )\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             compressed_docs = self.base_compressor.compress_documents(\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/retrievers/document_compressors/chain_filter.py\u001b[0m in \u001b[0;36mcompress_documents\u001b[0;34m(self, documents, query, callbacks)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRunnableConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         outputs = zip(\n\u001b[0;32m---> 60\u001b[0;31m             self.llm_chain.batch(\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             ),\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mbatch\u001b[0;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m   3168\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3169\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3170\u001b[0;31m                     inputs = step.batch(\n\u001b[0m\u001b[1;32m   3171\u001b[0m                         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3172\u001b[0m                         [\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mbatch\u001b[0;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mget_executor_for_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    617\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36m_wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         return super().map(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(input, config)\u001b[0m\n\u001b[1;32m    779\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0;31m# If there's only one input, don't bother with the executor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m         return cast(\n\u001b[1;32m    306\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    308\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    842\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                 results.append(\n\u001b[0;32m--> 683\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    684\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    909\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         }\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, reasoning_format, response_format, seed, service_tier, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    320\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         )\n\u001b[0;32m-> 1225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    918\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1006\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1006\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j52rz6n7emgtktsg2hy04h2x` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 5814, Requested 675. Please try again in 4.888s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
          ]
        }
      ]
    }
  ]
}