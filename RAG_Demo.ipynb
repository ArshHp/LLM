{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEos1XcEV41895DmCpcC3+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6dae9ee4da8c44fdac8c3d210cebb88f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64e1ddd5284241b1a6c88f3a2be6ff99",
              "IPY_MODEL_27bdb90b9b58439698249bedcf170919",
              "IPY_MODEL_d2ebfd729e8a48ff97ab4b3463535767"
            ],
            "layout": "IPY_MODEL_eaf93cdcef3748eb8af15249027f1b86"
          }
        },
        "64e1ddd5284241b1a6c88f3a2be6ff99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e07be4d27fc24a61a3549aa7a4523c63",
            "placeholder": "​",
            "style": "IPY_MODEL_12cdcba28fb54e559ce392664be984f9",
            "value": "Generating embeddings: 100%"
          }
        },
        "27bdb90b9b58439698249bedcf170919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b29ac01a422c45858889e77eddb4fac9",
            "max": 720,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94b587afe4f04c61b8139325efefa7d3",
            "value": 720
          }
        },
        "d2ebfd729e8a48ff97ab4b3463535767": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9555ff3c99764310ae3d34f6965a4ca0",
            "placeholder": "​",
            "style": "IPY_MODEL_ee2c3f590be7406cb37331b44c14d2ab",
            "value": " 720/720 [00:37&lt;00:00, 11.07it/s]"
          }
        },
        "eaf93cdcef3748eb8af15249027f1b86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e07be4d27fc24a61a3549aa7a4523c63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12cdcba28fb54e559ce392664be984f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b29ac01a422c45858889e77eddb4fac9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94b587afe4f04c61b8139325efefa7d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9555ff3c99764310ae3d34f6965a4ca0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee2c3f590be7406cb37331b44c14d2ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArshHp/LLM/blob/main/RAG_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIO4qfL7m-hn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "End to End RAG Pipeline using LLAMA Index & Langchain Framework"
      ],
      "metadata": {
        "id": "qDIyWaEOnE4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "vWQ21DIFnLHz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q python-dotenv llama-index sentence-transformers\n",
        "!pip install -q llama-index-embeddings-huggingface\n",
        "!pip install -q langchain-groq\n",
        "!pip install -q llama-index-llms-langchain\n",
        "!pip install -q langchain langchain_community\n",
        "!pip install -q google-generativeai\n",
        "!pip install -q llama-index.llms.gemini --upgrade --no-cache-dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFVtgt2vnU-X",
        "outputId": "bec7261d-584a-4419-ebe4-6110847cdb6d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.9/265.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.4/720.4 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the Env file\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv(), override=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87-PXxwLoiNt",
        "outputId": "4529464d-6fe4-4d8b-f200-030671815be0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Dataset\n",
        "TODO: Provide the browse option to load the files"
      ],
      "metadata": {
        "id": "_7ReeijPotdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LLM_MODEL=\"models/gemini-2.0-flash\"\n",
        "EMBEDDING_MODEL=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "DEVICE=\"cpu\" #TODO:\n",
        "#Add the code for if cuda exist else configue device as cpu, no need to setup torch now\n",
        "#if torch.cuda.is_available():\n",
        " #   DEVICE = \"cuda\"\n"
      ],
      "metadata": {
        "id": "sCw26Oj1ePjk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "import os\n",
        "\n",
        "# Configure Gemini LLM\n",
        "llm = Gemini(model= LLM_MODEL, api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "# Define the Hugging Face embedding model (assuming embed_model was defined previously)\n",
        "# If not, define it here:\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "embed_model = HuggingFaceEmbedding(\n",
        "model_name=EMBEDDING_MODEL,\n",
        "device=DEVICE  # or \"cuda\"\n",
        " )\n",
        "\n",
        "# Configure Settings to use the defined LLM and Embedding Model\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model # Use the embed_model defined earlier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "fZkkUpIQeLBO",
        "outputId": "bb4e721c-1274-48c4-9f3b-5f54987edb6b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-fca7a1567b33>:5: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
            "  llm = Gemini(model= LLM_MODEL, api_key=os.getenv(\"GOOGLE_API_KEY\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rTl-n19GeDbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "#To load the set of documents from a directory\n",
        "documents = SimpleDirectoryReader('/content/sample_data/data/').load_data()"
      ],
      "metadata": {
        "id": "CmZdQbSOo05E"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (documents[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRtjbhI2pezJ",
        "outputId": "39e13399-70d8-4186-dfd0-c8aa812a26b9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc ID: a3e8c883-dd7f-43da-8e70-1a1083c5fa8f\n",
            "Text: Figure 1: The Transformer - model architecture. The Transformer\n",
            "follows this overall architecture using stacked self-attention and\n",
            "point-wise, fully connected layers for both the encoder and decoder,\n",
            "shown in the left and right halves of Figure 1, respectively. 3.1\n",
            "Encoder and Decoder Stacks Encoder: The encoder is composed of a stack\n",
            "of N = 6 i...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking using Sentence Window Node Parser"
      ],
      "metadata": {
        "id": "cYRvzGoupizG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
        "sentence_window_parser = SentenceWindowNodeParser(\n",
        "     # how many sentences on either side to capture\n",
        "    window_size=3,\n",
        "\n",
        "    # the metadata key that holds the window of surrounding sentences\n",
        "    window_metadata_key=\"window\",\n",
        "\n",
        "    # the metadata key that holds the original sentence\n",
        "    original_text_metadata_key=\"original_sentence\",\n",
        ")\n",
        "nodes = sentence_window_parser.get_nodes_from_documents(documents)\n",
        "print (\"Total length of splitted nodes: \", len(nodes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK3mRV3ypoVU",
        "outputId": "e07d6976-18d4-43db-9e78-e2bc87d9970c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total length of splitted nodes:  720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets look at the chunks\n",
        "nodes[0].to_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HExco6RjpvtJ",
        "outputId": "7cc6662b-ea61-4322-a78a-5e6e6ba340ae"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id_': '473feefc-8927-4b43-a5b1-209c20963b56',\n",
              " 'embedding': None,\n",
              " 'metadata': {'page_label': '1',\n",
              "  'file_name': 'Attention_Paper.pdf',\n",
              "  'file_path': '/content/sample_data/data/Attention_Paper.pdf',\n",
              "  'file_type': 'application/pdf',\n",
              "  'file_size': 2215244,\n",
              "  'creation_date': '2025-05-27',\n",
              "  'last_modified_date': '2025-05-27',\n",
              "  'window': 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\n Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder.  The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism.  We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. ',\n",
              "  'original_sentence': 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\n'},\n",
              " 'excluded_embed_metadata_keys': ['file_name',\n",
              "  'file_type',\n",
              "  'file_size',\n",
              "  'creation_date',\n",
              "  'last_modified_date',\n",
              "  'last_accessed_date',\n",
              "  'window',\n",
              "  'original_sentence'],\n",
              " 'excluded_llm_metadata_keys': ['file_name',\n",
              "  'file_type',\n",
              "  'file_size',\n",
              "  'creation_date',\n",
              "  'last_modified_date',\n",
              "  'last_accessed_date',\n",
              "  'window',\n",
              "  'original_sentence'],\n",
              " 'relationships': {'1': {'node_id': '5425e1ad-a085-41e4-bde6-d5ffe8de4573',\n",
              "   'node_type': '4',\n",
              "   'metadata': {'page_label': '1',\n",
              "    'file_name': 'Attention_Paper.pdf',\n",
              "    'file_path': '/content/sample_data/data/Attention_Paper.pdf',\n",
              "    'file_type': 'application/pdf',\n",
              "    'file_size': 2215244,\n",
              "    'creation_date': '2025-05-27',\n",
              "    'last_modified_date': '2025-05-27'},\n",
              "   'hash': '6bc46dd2d875062d511b8d8f68f443cae7e95f5a38ad3a6fb5bef5936a9055ea',\n",
              "   'class_name': 'RelatedNodeInfo'},\n",
              "  '3': {'node_id': '0cec3273-f91a-4633-892e-4894591a0da1',\n",
              "   'node_type': '1',\n",
              "   'metadata': {'window': 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\n Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder.  The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism.  We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely.  Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. ',\n",
              "    'original_sentence': 'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. '},\n",
              "   'hash': '26fd53f434e8a12da48ba96f197f181bd46fb7f4eef6d7048c77310c7a2147e8',\n",
              "   'class_name': 'RelatedNodeInfo'}},\n",
              " 'metadata_template': '{key}: {value}',\n",
              " 'metadata_separator': '\\n',\n",
              " 'text': 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\n',\n",
              " 'mimetype': 'text/plain',\n",
              " 'start_char_idx': 0,\n",
              " 'end_char_idx': 174,\n",
              " 'metadata_seperator': '\\n',\n",
              " 'text_template': '{metadata_str}\\n\\n{content}',\n",
              " 'class_name': 'TextNode'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade --force-reinstall -q llama-index llama-index-vector-stores-chroma llama-index-embeddings-huggingface llama-index-llms-langchain langchain-groq chromadb\n",
        "#!pip install --upgrade --force-reinstall -q llama-index.llms.gemini --upgrade --no-cache-dir\n"
      ],
      "metadata": {
        "id": "Y1yeCkXOlGp-"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.storage.storage_context import StorageContext\n",
        "import chromadb\n",
        "\n",
        "#Setup ChromaDB persistent client\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")  # persists to disk\n",
        "chroma_collection = chroma_client.get_or_create_collection(\"rag_index\")\n",
        "\n",
        "#Setup Chroma Vector Store\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# Setup storage context and build the index\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "T9TlLug2cdsz",
        "outputId": "c5a421f7-5898-419b-c170-5b04c3a15abc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom llama_index.vector_stores.chroma import ChromaVectorStore\\nfrom llama_index.storage.storage_context import StorageContext\\nimport chromadb\\n\\n#Setup ChromaDB persistent client\\nchroma_client = chromadb.PersistentClient(path=\"./chroma_db\")  # persists to disk\\nchroma_collection = chroma_client.get_or_create_collection(\"rag_index\")\\n\\n#Setup Chroma Vector Store\\nvector_store = ChromaVectorStore(chroma_collection=chroma_collection)\\n\\n# Setup storage context and build the index\\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
        "\n",
        "#Build the indexing using the configured Settings\n",
        "#sentence_index = VectorStoreIndex(nodes,storage_context=storage_context,show_progress=True) # Settings are automatically picked up\n",
        "sentence_index = VectorStoreIndex(nodes,show_progress=True) # Settings are automatically picked up\n",
        "#sentence_index.storage_context.persist(persist_dir=\"./storage\")\n",
        "\n",
        "query_engine = sentence_index.as_query_engine(\n",
        "    similarity_top_k=2,\n",
        "    node_postprocessors=[\n",
        "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6dae9ee4da8c44fdac8c3d210cebb88f",
            "64e1ddd5284241b1a6c88f3a2be6ff99",
            "27bdb90b9b58439698249bedcf170919",
            "d2ebfd729e8a48ff97ab4b3463535767",
            "eaf93cdcef3748eb8af15249027f1b86",
            "e07be4d27fc24a61a3549aa7a4523c63",
            "12cdcba28fb54e559ce392664be984f9",
            "b29ac01a422c45858889e77eddb4fac9",
            "94b587afe4f04c61b8139325efefa7d3",
            "9555ff3c99764310ae3d34f6965a4ca0",
            "ee2c3f590be7406cb37331b44c14d2ab"
          ]
        },
        "id": "JCGXz_YCqB8Q",
        "outputId": "d1b8f199-2950-4e86-e90a-c91518dbde24"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/720 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6dae9ee4da8c44fdac8c3d210cebb88f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import load_index_from_storage\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.storage.storage_context import StorageContext\n",
        "import chromadb\n",
        "\n",
        "# Step 1: Connect to existing ChromaDB\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "chroma_collection = chroma_client.get_or_create_collection(\"rag_index\")\n",
        "\n",
        "# Step 2: Load vector store and storage context\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    vector_store=vector_store,\n",
        "    persist_dir=\"./storage\"\n",
        ")\n",
        "\n",
        "# Step 3: Reload the index\n",
        "index = load_index_from_storage(storage_context)\n",
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=2,\n",
        "    node_postprocessors=[\n",
        "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "yjJn5A9ndZ-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.response.notebook_utils import display_response\n",
        "sentence_window_response = query_engine.query(\"List the authors of attention paper?\")\n",
        "print (sentence_window_response.response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "74cmNpJOsvbI",
        "outputId": "87f790a1-cc9c-44df-cd8b-8a40c8374fee"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The authors mentioned in the provided text are: Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, Yonghui Wu, Łukasz Kaiser, Samy Bengio, Ofir Press, Lior Wolf, Rico Sennrich, Barry Haddow, Alexandra Birch, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print total nodes retrived\n",
        "print (\"Total Nodes Retrived:\",len(sentence_window_response.source_nodes) )\n",
        "for node in sentence_window_response.source_nodes:\n",
        "    print(\"\\n\\n---- Response Source Info ----\")\n",
        "    print(\"Retrived Text:\", node.node.get_content())\n",
        "    print(\"Metadata:\", node.node.metadata)\n",
        "    print(\"Nodes Score:\", node.score)\n",
        "    print(\"Additional Content:\", node.node.metadata[\"window\"])\n",
        "    print(\"-------------------------------\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukx1sbnEWoWa",
        "outputId": "125e1f52-60d2-468b-8904-38bf2480bcd7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Nodes Retrived: 2\n",
            "\n",
            "\n",
            "---- Response Source Info ----\n",
            "Retrived Text: Self-training PCFG grammars with latent annotations\n",
            "across languages.  In Proceedings of the 2009 Conference on Empirical Methods in Natural\n",
            "Language Processing, pages 832–841.  ACL, August 2009.\n",
            " [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.  Exploring\n",
            "the limits of language modeling.  arXiv preprint arXiv:1602.02410, 2016.\n",
            " [16] Łukasz Kaiser and Samy Bengio. \n",
            "Metadata: {'page_label': '11', 'file_name': 'Attention_Paper.pdf', 'file_path': '/content/sample_data/data/Attention_Paper.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-05-27', 'last_modified_date': '2025-05-27', 'window': 'Self-training PCFG grammars with latent annotations\\nacross languages.  In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841.  ACL, August 2009.\\n [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.  Exploring\\nthe limits of language modeling.  arXiv preprint arXiv:1602.02410, 2016.\\n [16] Łukasz Kaiser and Samy Bengio. ', 'original_sentence': '[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. '}\n",
            "Nodes Score: 0.6183107104538762\n",
            "Additional Content: Self-training PCFG grammars with latent annotations\n",
            "across languages.  In Proceedings of the 2009 Conference on Empirical Methods in Natural\n",
            "Language Processing, pages 832–841.  ACL, August 2009.\n",
            " [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.  Exploring\n",
            "the limits of language modeling.  arXiv preprint arXiv:1602.02410, 2016.\n",
            " [16] Łukasz Kaiser and Samy Bengio. \n",
            "-------------------------------\n",
            "\n",
            "\n",
            "---- Response Source Info ----\n",
            "Retrived Text: [30] Ofir Press and Lior Wolf.  Using the output embedding to improve language models.  arXiv\n",
            "preprint arXiv:1608.05859, 2016.\n",
            " [31] Rico Sennrich, Barry Haddow, and Alexandra Birch.  Neural machine translation of rare words\n",
            "with subword units.  arXiv preprint arXiv:1508.07909, 2015.\n",
            " [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\n",
            "and Jeff Dean. \n",
            "Metadata: {'page_label': '12', 'file_name': 'Attention_Paper.pdf', 'file_path': '/content/sample_data/data/Attention_Paper.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-05-27', 'last_modified_date': '2025-05-27', 'window': '[30] Ofir Press and Lior Wolf.  Using the output embedding to improve language models.  arXiv\\npreprint arXiv:1608.05859, 2016.\\n [31] Rico Sennrich, Barry Haddow, and Alexandra Birch.  Neural machine translation of rare words\\nwith subword units.  arXiv preprint arXiv:1508.07909, 2015.\\n [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. ', 'original_sentence': '[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. '}\n",
            "Nodes Score: 0.6043808202453456\n",
            "Additional Content: [30] Ofir Press and Lior Wolf.  Using the output embedding to improve language models.  arXiv\n",
            "preprint arXiv:1608.05859, 2016.\n",
            " [31] Rico Sennrich, Barry Haddow, and Alexandra Birch.  Neural machine translation of rare words\n",
            "with subword units.  arXiv preprint arXiv:1508.07909, 2015.\n",
            " [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\n",
            "and Jeff Dean. \n",
            "-------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_window_response = query_engine.query(\"What is multi-head attention\")\n",
        "print (sentence_window_response.response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "qLbkw6Goqhwc",
        "outputId": "33edafff-f624-4ea8-e735-e040c1efba89"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It consists of several attention layers running in parallel. The Transformer uses it in three different ways: in \"encoder-decoder attention\" layers, in the encoder with self-attention layers.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_window_response = query_engine.query(\"Provide an elboratove answer for What is multi-head attention\")\n",
        "print (sentence_window_response.response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "EBoLsfrgquzE",
        "outputId": "083ed748-6f02-489a-e9ee-d8df47de68a7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-Head Attention involves multiple attention layers that operate in parallel. It is used to counteract the reduced effective resolution caused by averaging attention-weighted positions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_window_response = query_engine.query(\"What are the minimum installment amount for monthly, Half-Yearly, Quaterly and Yearly payments?\")\n",
        "print (sentence_window_response.response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "jIZ245b9q1gQ",
        "outputId": "1f5d4ee9-59da-4c21-8274-5b5805ef5ba0"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The minimum installment amounts are: Rs. 5000 for monthly, Rs. 15000 for quarterly, Rs. 25000 for half-yearly, and Rs. 50000 for yearly payments.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn it into a chat engine\n",
        "# Create a query engine\n",
        "chat_engine  = sentence_index.as_chat_engine(\n",
        "    chat_mode=\"condense_question\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Simulate chat\n",
        "print(\"Ask a question (type 'exit' to quit):\")\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        break\n",
        "    response = chat_engine.chat(user_input)\n",
        "    print(\"Bot:\", response.response)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "LacdXXCxsZrx",
        "outputId": "6b56f775-125d-4a47-8ef2-a2f658615bfa"
      },
      "execution_count": 58,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask a question (type 'exit' to quit):\n",
            "User: What is Attention \n",
            "Querying with: What is Attention \n",
            "Bot: The document discusses attention distributions and expresses excitement about the future of attention-based models, with plans to apply them to other tasks.\n",
            "\n",
            "User: Tell me more\n",
            "Querying with: The document discusses attention distributions and expresses excitement about the future of attention-based models, with plans to apply them to other tasks. Can you tell me more about this?\n",
            "\n",
            "Bot: The document includes a discussion and presentation of attention distribution examples in the appendix. There is also excitement expressed about the future of attention-based models, with intentions to use them on other tasks.\n",
            "\n",
            "User: What is the main principal behind it\n",
            "Querying with: Based on the document previously discussed, which mentions attention distributions and excitement about applying attention-based models to other tasks, what is the main principle behind attention?\n",
            "\n",
            "Bot: Attention distributions are inspected from the models, and there are plans to apply attention-based models to other tasks.\n",
            "\n",
            "User: quit\n",
            "Querying with: Given the previous discussion about attention distributions and the application of attention-based models, is there a main principle behind attention?\n",
            "\n",
            "Bot: The future of attention-based models is exciting, and there are plans to apply them to other tasks. Attention distributions from models have been examined, and examples are presented and discussed in the appendix.\n",
            "\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Request\n",
        "from pydantic import BaseModel\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "class ChatInput(BaseModel):\n",
        "    message: str\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat_endpoint(input: ChatInput):\n",
        "    response = chat_engine.chat(input.message)\n",
        "    return {\"response\": response.response}\n"
      ],
      "metadata": {
        "id": "r96TwWlZr-Gy"
      },
      "execution_count": 59,
      "outputs": []
    }
  ]
}